# Module 2: Model Deployment

**Ship models via batch jobs, web APIs, and streaming services**

## ğŸ¯ Learning Objectives

By the end of this module, you will be able to:

- Build production-ready REST APIs for model serving
- Deploy models as serverless functions (AWS Lambda)
- Implement batch inference jobs
- Build real-time inference pipelines with streaming
- Handle model versioning and A/B testing

## ğŸ“‹ Topics Covered

### 1. REST API Deployment (FastAPI)
- Building RESTful APIs for ML models
- Request/response validation
- Authentication and rate limiting
- Containerization and deployment

### 2. Serverless Deployment (AWS Lambda)
- Deploying models as Lambda functions
- Handling cold starts
- API Gateway integration
- Serverless best practices

### 3. Batch Inference
- Designing batch processing pipelines
- AWS Batch for ML workloads
- Scalable data processing patterns

### 4. Real-Time Streaming
- Building streaming inference pipelines
- AWS Kinesis integration
- Low-latency model serving

## ğŸ“‚ Module Structure

```
module-02/
â”œâ”€â”€ batch-api/
â”‚   â”œâ”€â”€ fastapi/          # REST API deployment with FastAPI
â”‚   â””â”€â”€ batch-lambda/     # Batch processing with AWS Lambda
â”œâ”€â”€ streaming/
â”‚   â”œâ”€â”€ kinesis/          # AWS Kinesis streaming pipeline
â”‚   â””â”€â”€ real-time/        # Real-time inference patterns
â”œâ”€â”€ exercises/            # Hands-on practice exercises
â””â”€â”€ solution/             # Exercise solutions
```

## ğŸš€ Getting Started

### Prerequisites

- Completed Module 1: Infrastructure & Prerequisites
- Docker installed and running
- AWS account with appropriate permissions
- Basic knowledge of REST APIs

### Setup

1. Navigate to the batch-api directory:
   ```bash
   cd module-02/batch-api
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the FastAPI server:
   ```bash
   uvicorn main:app --reload
   ```

## ğŸ“– Lessons

### Lesson 2.1: REST API with FastAPI
Build a production-ready API for model serving.

- [FastAPI Guide](./batch-api/fastapi/README.md)
- Exercise: Build a model inference API

### Lesson 2.2: Serverless Deployment
Deploy your model as an AWS Lambda function.

- [Lambda Guide](./batch-api/batch-lambda/README.md)
- Exercise: Deploy serverless model endpoint

### Lesson 2.3: Streaming Inference
Build real-time inference with AWS Kinesis.

- [Kinesis Guide](./streaming/kinesis/README.md)
- Exercise: Create streaming ML pipeline

## ğŸ’¡ Deployment Patterns

### Synchronous REST API
```
Client â†’ API Gateway â†’ Load Balancer â†’ Container/Service â†’ Model
                      â† Response â†                  â† Prediction
```

### Serverless
```
Client â†’ API Gateway â†’ Lambda â†’ Model (in container or package)
                      â† Response â†
```

### Batch Processing
```
S3 â†’ SQS â†’ Lambda/Batch â†’ Model â†’ Results S3
                     â†“
                  CloudWatch
```

### Streaming
```
Producer â†’ Kinesis Stream â†’ Lambda/EC2 â†’ Model â†’ Kinesis Firehose â†’ S3/DynamoDB
```

## ğŸ“š Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/)
- [AWS Kinesis Documentation](https://docs.aws.amazon.com/kinesis/)
- [Serverless Framework](https://www.serverless.com/framework)

## ğŸ“ Next Steps

After completing this module, proceed to [Module 3: Testing & CI/CD](../module-03/README.md)
